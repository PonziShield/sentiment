{"cells":[{"cell_type":"markdown","metadata":{"id":"xNOhIrG5odvk"},"source":["#### Implementation"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"AhNIvoNPciux","executionInfo":{"status":"ok","timestamp":1702834079475,"user_tz":-330,"elapsed":12791,"user":{"displayName":"Nimsara Fernando","userId":"05456346427770267598"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import RobertaModel, RobertaTokenizer\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","from imblearn.under_sampling import RandomUnderSampler\n","from collections import Counter\n","from torch.optim.lr_scheduler import ExponentialLR\n","from sklearn.model_selection import train_test_split\n","\n","import re\n","from bs4 import BeautifulSoup\n","from nltk.tokenize import WordPunctTokenizer"]},{"cell_type":"markdown","metadata":{"id":"foX6--B_j_HP"},"source":["----------------------------------------------------------\n","To fix the error `Torch compile: libcuda.so cannot found` raised by\n","```python\n","torch.compile(robertaModel, backend=\"inductor\")\n","```\n","----------------------------------------------------------"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"4doL32HysyJB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702834099971,"user_tz":-330,"elapsed":20504,"user":{"displayName":"Nimsara Fernando","userId":"05456346427770267598"}},"outputId":"1359a96c-cefe-48fb-9917-468f79f8e1c8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3718,"status":"ok","timestamp":1702834208029,"user":{"displayName":"Nimsara Fernando","userId":"05456346427770267598"},"user_tz":-330},"id":"0rn8p5YLjxRh","outputId":"6b1eed7b-03e6-46d3-913b-06cfa5c12c28"},"outputs":[{"output_type":"stream","name":"stdout","text":["/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n"]}],"source":["!export LC_ALL=\"en_US.UTF-8\"\n","!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n","!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n","!ldconfig /usr/lib64-nvidia"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":518,"status":"ok","timestamp":1702834123593,"user":{"displayName":"Nimsara Fernando","userId":"05456346427770267598"},"user_tz":-330},"id":"lLg3U4RPcPYb","outputId":"adb72bb3-0d7b-4555-bb58-3fa4e09ebd8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["# Check for GPU availability\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"KwS9kluUbLrU"},"source":["#### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qr5FWL_cbM_D"},"outputs":[],"source":["file_path = '/content/drive/Shareddrives/test/Sentiment/Tweets.csv'\n","\n","df = pd.read_csv(file_path, usecols=['airline_sentiment', 'text'])\n","df.dropna(subset=['text'], inplace=True)\n","df.dropna(subset=['airline_sentiment'], inplace=True)\n","\n","df['airline_sentiment'] = df['airline_sentiment'].map({'neutral': 2, 'positive': 1, 'negative': 0})\n","\n","valid_sentiments = [0, 1]\n","df_valid = df[df['airline_sentiment'].isin(valid_sentiments)]"]},{"cell_type":"code","source":["tok = WordPunctTokenizer()\n","\n","pat1 = r'@[A-Za-z0-9_]+'\n","pat2 = r'https?://[^ ]+'\n","combined_pat = r'|'.join((pat1, pat2))\n","www_pat = r'www.[^ ]+'\n","negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n","                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n","                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n","                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n","                \"mustn't\":\"must not\"}\n","neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n","\n","def tweet_cleaner_updated(text):\n","    soup = BeautifulSoup(text, 'lxml')\n","    souped = soup.get_text()\n","    try:\n","        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n","    except:\n","        bom_removed = souped\n","    stripped = re.sub(combined_pat, '', bom_removed)\n","    stripped = re.sub(www_pat, '', stripped)\n","    lower_case = stripped.lower()\n","    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n","    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n","    # During the letters_only process two lines above, it has created unnecessay white spaces,\n","    # I will tokenize and join together to remove unneccessary white spaces\n","    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n","    return (\" \".join(words)).strip()"],"metadata":{"id":"qY_lxxTl-EJE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5wH8wtV4bRGw"},"outputs":[],"source":["x = df_valid.text\n","# x = df_valid.text.map(lambda text: tweet_cleaner_updated(text))\n","y = df_valid.airline_sentiment\n","y = y.astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4557,"status":"ok","timestamp":1702787635465,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"},"user_tz":-330},"id":"ZyxS3qw0bUM6","outputId":"eaa9978d-2857-429c-adf9-b4fde7b3f270"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original class distribution summary: Counter({0: 9178, 1: 2363})\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-17-42d241614f98>:15: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  soup = BeautifulSoup(text, 'lxml')\n"]},{"output_type":"stream","name":"stdout","text":["Downsampled class distribution summary: Counter({0: 2363, 1: 2363})\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-23-74e20fa122dd>:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_valid['cleaned_text'] = df_valid['text'].apply(tweet_cleaner_updated)\n"]}],"source":["# Print Class Distribution Summary\n","print('Original class distribution summary: {}'.format(Counter(df_valid['airline_sentiment'])))\n","\n","# Downsample Majority Class\n","rus = RandomUnderSampler(sampling_strategy='auto', random_state=2023)\n","df_valid['cleaned_text'] = df_valid['text'].apply(tweet_cleaner_updated)\n","# X_resampled, y_resampled = rus.fit_resample(df_valid[['text']], df_valid['airline_sentiment'])\n","X_resampled, y_resampled = rus.fit_resample(df_valid[['cleaned_text']], df_valid['airline_sentiment'])\n","\n","# Create a new DataFrame with resampled data\n","df_resampled = pd.DataFrame({'text': X_resampled.squeeze(), 'airline_sentiment': y_resampled})\n","\n","# Print Downsampled Class Distribution Summary\n","print('Downsampled class distribution summary: {}'.format(Counter(df_resampled['airline_sentiment'])))\n","\n","\n","# Now, df_resampled contains the downsampled dataset\n","x = df_resampled['text'].tolist()\n","y = df_resampled['airline_sentiment'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"elapsed":380,"status":"ok","timestamp":1702787740865,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"},"user_tz":-330},"id":"r6q8-zMsbWRE","outputId":"35854e81-f7be-4ad4-f405-0ec6939742e9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                text  airline_sentiment\n","0  she could even see that had tried to make the ...                  0\n","1  the pilot told us they would release bags as w...                  0\n","2        jp dm message who can not get dm through to                  0\n","3  hr min cost of flight change was hrs ago drop ...                  0\n","4  we saw one he was as useless as the tsa agents...                  0"],"text/html":["\n","  <div id=\"df-6e0fc4ba-cbea-4273-b3ad-84df254125b6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>airline_sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>she could even see that had tried to make the ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>the pilot told us they would release bags as w...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>jp dm message who can not get dm through to</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>hr min cost of flight change was hrs ago drop ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>we saw one he was as useless as the tsa agents...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e0fc4ba-cbea-4273-b3ad-84df254125b6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6e0fc4ba-cbea-4273-b3ad-84df254125b6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6e0fc4ba-cbea-4273-b3ad-84df254125b6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b837f9eb-b38b-4cb5-9c7e-612937cb2e5d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b837f9eb-b38b-4cb5-9c7e-612937cb2e5d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b837f9eb-b38b-4cb5-9c7e-612937cb2e5d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":26}],"source":["df_resampled.head()"]},{"cell_type":"markdown","metadata":{"id":"4uun-V2hbW-b"},"source":["#### Implementation"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ud9wV2k5oY7w","executionInfo":{"status":"ok","timestamp":1702834129983,"user_tz":-330,"elapsed":1130,"user":{"displayName":"Nimsara Fernando","userId":"05456346427770267598"}}},"outputs":[],"source":["class SentimentAnalysisModel(nn.Module):\n","    def __init__(\n","        self,\n","        device,\n","        roberta_model_path='roberta-base',\n","        input_dim=512,\n","        units=256,\n","        channels=1,\n","        conv_size=32,\n","        kernel_size1=4,\n","        kernel_size2=6,\n","        dense_units=256,\n","        num_classes=1,\n","        inductor=True\n","        ):\n","        super(SentimentAnalysisModel, self).__init__()\n","\n","        # Load pre-trained RoBERTa model\n","        self.roberta = RobertaModel.from_pretrained(roberta_model_path).to(device=device)\n","        if (inductor):\n","          self.roberta = torch.compile(self.roberta, backend=\"inductor\")\n","        self.tokenizer = RobertaTokenizer.from_pretrained(roberta_model_path)\n","\n","        # GRU branch\n","        self.gru = nn.GRU(input_dim, units, bidirectional=True, batch_first=True).to(device=device)\n","        self.attention_gru = AttentionWithContext(device, units * 2)\n","\n","        # LSTM branch\n","        self.lstm = nn.LSTM(input_dim, units, bidirectional=True, batch_first=True).to(device=device)\n","        self.attention_lstm = AttentionWithContext(device, units * 2)\n","\n","        # CNN branches\n","        self.channels = channels\n","        self.cnn1 = nn.Conv1d(channels, conv_size, kernel_size=kernel_size1, padding=0).to(device=device)\n","        self.cnn2 = nn.Conv1d(channels, conv_size, kernel_size=kernel_size2, padding=0).to(device=device)\n","\n","        # Pooling layers\n","        self.avg_pool = nn.AdaptiveAvgPool1d(1).to(device=device)\n","        self.max_pool = nn.AdaptiveMaxPool1d(1).to(device=device)\n","\n","        # Normalization\n","        self.norm = nn.BatchNorm1d(2 * 4 * conv_size).to(device=device)\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(2 * 4 * conv_size, dense_units).to(device=device)\n","        self.relu = nn.ReLU().to(device=device)\n","        self.fc2 = nn.Linear(256, num_classes).to(device=device)\n","        self.sigmoid = nn.Sigmoid().to(device=device)\n","\n","    def forward(self, x):\n","        # Tokenize and encode the sentences\n","        # tokenized_sentences = self.tokenizer(x, padding='max_length', return_tensors='pt').to(device=device)\n","        tokenized_sentences = self.tokenizer(x, truncation=True, padding='max_length', return_tensors='pt').to(device=device)\n","        # print('tokenized_sentences', tokenized_sentences.shape)\n","\n","        # Forward pass to get embeddings\n","        with torch.no_grad():\n","            # Get RoBERTa embeddings\n","            model_output = self.roberta(**tokenized_sentences)\n","\n","        # Extract embeddings from the output\n","        embeddings = model_output.last_hidden_state\n","        # print('embeddings', embeddings.shape)\n","\n","        # GRU branch\n","        gru_out, _ = self.gru(embeddings.permute(0, 2, 1))\n","        # print('gru_out', gru_out.shape)\n","        gru_attention = self.attention_gru(gru_out)\n","        # print('gru_attention', gru_attention.shape)\n","\n","        # LSTM branch\n","        lstm_out, _ = self.lstm(embeddings.permute(0, 2, 1))\n","        # print('lstm_out', lstm_out.shape)\n","        lstm_attention = self.attention_lstm(lstm_out)\n","        # print('lstm_attention', lstm_attention.shape)\n","\n","        # Expand before CNN\n","        gru_attention_expand = gru_attention.unsqueeze(1).expand(-1, self.channels, -1)\n","        # print('gru_attention_expand', gru_attention_expand.shape)\n","        lstm_attention_expand = lstm_attention.unsqueeze(1).expand(-1, self.channels, -1)\n","        # print('lstm_attention_expand', lstm_attention_expand.shape)\n","\n","        # CNN branches\n","        gru_cnn1_out = self.cnn1(gru_attention_expand)\n","        # print('gru_cnn1_out', gru_cnn1_out.shape)\n","        gru_cnn2_out = self.cnn2(gru_attention_expand)\n","        # print('gru_cnn2_out', gru_cnn2_out.shape)\n","        lstm_cnn1_out = self.cnn1(lstm_attention_expand)\n","        # print('lstm_cnn1_out', lstm_cnn1_out.shape)\n","        lstm_cnn2_out = self.cnn2(lstm_attention_expand)\n","        # print('lstm_cnn2_out', lstm_cnn2_out.shape)\n","\n","        # Pooling\n","        gru_cnn1_avg_pool = self.avg_pool(gru_cnn1_out)\n","        # print('gru_cnn1_avg_pool', gru_cnn1_avg_pool.shape)\n","        gru_cnn1_max_pool = self.max_pool(gru_cnn1_out)\n","        # print('gru_cnn1_max_pool', gru_cnn1_max_pool.shape)\n","        gru_cnn2_avg_pool = self.avg_pool(gru_cnn2_out)\n","        # print('gru_cnn2_avg_pool', gru_cnn2_avg_pool.shape)\n","        gru_cnn2_max_pool = self.max_pool(gru_cnn2_out)\n","        # print('gru_cnn2_max_pool', gru_cnn2_max_pool.shape)\n","\n","        lstm_cnn1_avg_pool = self.avg_pool(lstm_cnn1_out)\n","        # print('lstm_cnn1_avg_pool', lstm_cnn1_avg_pool.shape)\n","        lstm_cnn1_max_pool = self.max_pool(lstm_cnn1_out)\n","        # print('lstm_cnn1_max_pool', lstm_cnn1_max_pool.shape)\n","        lstm_cnn2_avg_pool = self.avg_pool(lstm_cnn2_out)\n","        # print('lstm_cnn2_avg_pool', lstm_cnn2_avg_pool.shape)\n","        lstm_cnn2_max_pool = self.max_pool(lstm_cnn2_out)\n","        # print('lstm_cnn2_max_pool', lstm_cnn2_max_pool.shape)\n","\n","        # Concatenate and normalize\n","        concatenated = torch.cat(\n","            [\n","                gru_cnn1_avg_pool,\n","                gru_cnn1_max_pool,\n","                gru_cnn2_avg_pool,\n","                gru_cnn2_max_pool,\n","                lstm_cnn1_avg_pool,\n","                lstm_cnn1_max_pool,\n","                lstm_cnn2_avg_pool,\n","                lstm_cnn2_max_pool\n","            ],\n","             dim=1\n","            )\n","        # print('concatenated', concatenated.shape)\n","        normalized = self.norm(concatenated.view(concatenated.size(0), -1))\n","        # print('normalized', normalized.shape)\n","\n","        # Fully connected layers\n","        fc1_out = self.fc1(normalized)\n","        # print('fc1_out', fc1_out.shape)\n","        fc1_relu = self.relu(fc1_out)\n","        # print('fc1_relu', fc1_relu.shape)\n","        fc2_out = self.fc2(fc1_relu)\n","        # print('fc2_out', fc2_out.shape)\n","        output = self.sigmoid(fc2_out)\n","        # print('output', output.shape)\n","\n","        # Delete the embeddings tensor to release memory\n","        del embeddings\n","\n","        return output\n","\n","class AttentionWithContext(nn.Module):\n","    def __init__(self, device, hidden_size):\n","        super(AttentionWithContext, self).__init__()\n","\n","        # Linear layers for attention\n","        self.W_query = nn.Linear(hidden_size, hidden_size).to(device=device)\n","        self.W_context = nn.Linear(hidden_size, hidden_size).to(device=device)\n","        self.V = nn.Linear(hidden_size, 1).to(device=device)\n","\n","    def forward(self, lstm_output):\n","        # Compute query and context representations\n","        query = torch.tanh(self.W_query(lstm_output))\n","        context = torch.tanh(self.W_context(lstm_output))\n","\n","        # Compute attention scores using dot-product attention\n","        attention_scores = self.V(query).squeeze(-1)\n","        attention_weights = F.softmax(attention_scores, dim=-1)\n","\n","        # Apply attention weights to the LSTM output to get the context vector\n","        context_vector = torch.sum(attention_weights.unsqueeze(-1) * context, dim=1)\n","\n","        return context_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6QKQK1evWHg","executionInfo":{"status":"ok","timestamp":1702786888417,"user_tz":-330,"elapsed":1721951,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}},"outputId":"d8e52126-abe3-40db-f40c-85d72947b0bb"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 0.6845, Training Accuracy: 0.6392, Validation Accuracy: 0.4937\n","Epoch 2/10, Loss: 0.9682, Training Accuracy: 0.7111, Validation Accuracy: 0.4958\n","Epoch 3/10, Loss: 1.4524, Training Accuracy: 0.7545, Validation Accuracy: 0.5169\n","Epoch 4/10, Loss: 0.4328, Training Accuracy: 0.7966, Validation Accuracy: 0.7992\n","Epoch 5/10, Loss: 0.5670, Training Accuracy: 0.8198, Validation Accuracy: 0.6892\n","Epoch 6/10, Loss: 0.3999, Training Accuracy: 0.8384, Validation Accuracy: 0.8203\n","Epoch 7/10, Loss: 0.4350, Training Accuracy: 0.8447, Validation Accuracy: 0.7833\n","Epoch 8/10, Loss: 0.4940, Training Accuracy: 0.8521, Validation Accuracy: 0.7643\n","Epoch 9/10, Loss: 0.4110, Training Accuracy: 0.8630, Validation Accuracy: 0.8182\n","Epoch 10/10, Loss: 0.5051, Training Accuracy: 0.8646, Validation Accuracy: 0.7801\n"]}],"source":["sentences = [\"I love this product!\", \"It's terrible.\", \"Awesome experience.\", \"Worst ever.\", \"Great job!\", \"Awful.\", \"Excellent service.\", \"Hate it.\", \"Fantastic!\", \"Disappointing.\"]\n","labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n","\n","# Split the data into training and validation sets\n","x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=2023)\n","\n","# Create DataLoader for training and validation sets\n","batch_size = 128\n","\n","train_dataset = list(zip(x_train, y_train))\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","val_dataset = list(zip(x_val, y_val))\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Initialize the model\n","model = SentimentAnalysisModel(device)\n","\n","# Loss function and optimizer\n","criterion = nn.BCELoss().to(device=device)  # Binary Cross-Entropy Loss for binary classification\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","d_r = 10**-10\n","scheduler = ExponentialLR(optimizer, gamma=(1.0 - d_r))\n","\n","# Define early stopping parameters\n","patience = 5\n","best_validation_accuracy = 0\n","no_improvement_counter = 0\n","\n","# Training loop\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_correct_train = 0\n","    total_samples_train = 0\n","\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","\n","        # Compute training accuracy\n","        predicted_labels_train = (outputs > 0.5).int()\n","        total_correct_train += (predicted_labels_train.view(-1) == labels.to(device=device)).sum().item()\n","\n","        # Accumulate the sum of batch sizes\n","        total_samples_train += len(labels)\n","\n","        loss = criterion(outputs, torch.as_tensor(labels, dtype=torch.float32).unsqueeze(1).to(device=device))\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Compute training accuracy\n","    train_accuracy = total_correct_train / total_samples_train\n","\n","    # Validation\n","    val_loss = 0.0\n","    total_correct_val = 0\n","    total_samples_val = 0\n","    num_iterations = (len(y_val) + batch_size - 1) // batch_size\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for val_inputs, val_labels in val_loader:\n","            val_outputs = model(val_inputs)\n","\n","            # Compute validation accuracy\n","            predicted_labels_val = (val_outputs > 0.5).int()\n","            total_correct_val += (predicted_labels_val.view(-1) == val_labels.to(device=device)).sum().item()\n","\n","            # Accumulate the sum of batch sizes\n","            total_samples_val += len(val_labels)\n","\n","            val_batch_targets = torch.as_tensor(val_labels, dtype=torch.float32).unsqueeze(1).to(device=device)\n","            ce = criterion(val_outputs, val_batch_targets).item()\n","            val_loss += ce\n","\n","        avg_val_loss = val_loss / len(val_loader)\n","        val_accuracy = total_correct_val / total_samples_val\n","\n","        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_val_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","        # Check for early stopping\n","        if val_accuracy > best_validation_accuracy:\n","            best_validation_accuracy = val_accuracy\n","            no_improvement_counter = 0\n","            # Save the trained best model if needed\n","            torch.save(model.state_dict(), 'sentiment-analysis-twitterus-model.pth')\n","        else:\n","            no_improvement_counter += 1\n","\n","        # If no improvement for 'patience' consecutive epochs, stop training\n","        if no_improvement_counter >= patience:\n","            print(\"Early stopping triggered. Training stopped.\")\n","            break"]},{"cell_type":"code","source":["# Create an instance of the model\n","twitterusModel = SentimentAnalysisModel(device)\n","\n","# Load the saved model state dictionary\n","twitterusModel.load_state_dict(torch.load('/content/drive/Shareddrives/test/FYP/sentiment/sentiment-analysis-twitterus-model.pth'))\n","\n","# Validation\n","val_loss = 0.0\n","total_correct_val = 0\n","total_samples_val = 0\n","num_iterations = (len(y_val) + batch_size - 1) // batch_size\n","\n","twitterusModel.eval()\n","with torch.no_grad():\n","    for val_inputs, val_labels in val_loader:\n","        val_outputs = twitterusModel(val_inputs)\n","\n","        # Compute validation accuracy\n","        predicted_labels_val = (val_outputs > 0.5).int()\n","        total_correct_val += (predicted_labels_val.view(-1) == val_labels.to(device=device)).sum().item()\n","\n","        # Accumulate the sum of batch sizes\n","        total_samples_val += len(val_labels)\n","\n","        val_batch_targets = torch.as_tensor(val_labels, dtype=torch.float32).unsqueeze(1).to(device=device)\n","        ce = criterion(val_outputs, val_batch_targets).item()\n","        val_loss += ce\n","\n","    avg_val_loss = val_loss / len(val_loader)\n","    val_accuracy = total_correct_val / total_samples_val\n","\n","    print(f'Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPS8tYk1ANvr","executionInfo":{"status":"ok","timestamp":1702787284625,"user_tz":-330,"elapsed":54871,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}},"outputId":"1bc65ae0-0441-470e-fc3d-1708e43908a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Loss: 0.3999, Validation Accuracy: 0.8203\n"]}]},{"cell_type":"code","source":["sentences = [\"I love this product!\", \"It's terrible.\", \"Awesome experience.\", \"Worst ever.\", \"Great job!\", \"Awful.\", \"Excellent service.\", \"Hate it.\", \"Fantastic!\", \"Disappointing.\"]\n","labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n","\n","# Split the data into training and validation sets\n","x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.2, random_state=2023)\n","x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=2023)\n","\n","# Create DataLoader for training and validation sets\n","batch_size = 128\n","\n","train_dataset = list(zip(x_train, y_train))\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","val_dataset = list(zip(x_val, y_val))\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Initialize the model\n","model = SentimentAnalysisModel(device)\n","\n","# Loss function and optimizer\n","criterion = nn.BCELoss().to(device=device)  # Binary Cross-Entropy Loss for binary classification\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","d_r = 10**-10\n","scheduler = ExponentialLR(optimizer, gamma=(1.0 - d_r))\n","\n","# Define early stopping parameters\n","patience = 5\n","best_validation_accuracy = 0\n","no_improvement_counter = 0\n","\n","# Training loop\n","num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_correct_train = 0\n","    total_samples_train = 0\n","\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","\n","        # Compute training accuracy\n","        predicted_labels_train = (outputs > 0.5).int()\n","        total_correct_train += (predicted_labels_train.view(-1) == labels.to(device=device)).sum().item()\n","\n","        # Accumulate the sum of batch sizes\n","        total_samples_train += len(labels)\n","\n","        loss = criterion(outputs, torch.as_tensor(labels, dtype=torch.float32).unsqueeze(1).to(device=device))\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Compute training accuracy\n","    train_accuracy = total_correct_train / total_samples_train\n","\n","    # Validation\n","    val_loss = 0.0\n","    total_correct_val = 0\n","    total_samples_val = 0\n","    num_iterations = (len(y_val) + batch_size - 1) // batch_size\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for val_inputs, val_labels in val_loader:\n","            val_outputs = model(val_inputs)\n","\n","            # Compute validation accuracy\n","            predicted_labels_val = (val_outputs > 0.5).int()\n","            total_correct_val += (predicted_labels_val.view(-1) == val_labels.to(device=device)).sum().item()\n","\n","            # Accumulate the sum of batch sizes\n","            total_samples_val += len(val_labels)\n","\n","            val_batch_targets = torch.as_tensor(val_labels, dtype=torch.float32).unsqueeze(1).to(device=device)\n","            ce = criterion(val_outputs, val_batch_targets).item()\n","            val_loss += ce\n","\n","        avg_val_loss = val_loss / len(val_loader)\n","        val_accuracy = total_correct_val / total_samples_val\n","\n","        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_val_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","        # Check for early stopping\n","        if val_accuracy > best_validation_accuracy:\n","            best_validation_accuracy = val_accuracy\n","            no_improvement_counter = 0\n","            # Save the trained best model if needed\n","            torch.save(model.state_dict(), '/content/drive/Shareddrives/test/FYP/sentiment/sentiment-analysis-twitterus-all-model.pth')\n","        else:\n","            no_improvement_counter += 1\n","\n","        # If no improvement for 'patience' consecutive epochs, stop training\n","        if no_improvement_counter >= patience:\n","            print(\"Early stopping triggered. Training stopped.\")\n","            break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9bLv1v4UDfh-","outputId":"386da3ee-8f82-4689-cfe0-4652902065d3","executionInfo":{"status":"ok","timestamp":1702799040446,"user_tz":-330,"elapsed":4036863,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Loss: 0.4935, Training Accuracy: 0.7968, Validation Accuracy: 0.8043\n","Epoch 2/20, Loss: 0.4321, Training Accuracy: 0.8575, Validation Accuracy: 0.8398\n","Epoch 3/20, Loss: 0.7309, Training Accuracy: 0.8720, Validation Accuracy: 0.7446\n","Epoch 4/20, Loss: 0.4609, Training Accuracy: 0.8805, Validation Accuracy: 0.8268\n","Epoch 5/20, Loss: 0.3473, Training Accuracy: 0.8956, Validation Accuracy: 0.8563\n","Epoch 6/20, Loss: 0.3180, Training Accuracy: 0.8990, Validation Accuracy: 0.8814\n","Epoch 7/20, Loss: 0.2836, Training Accuracy: 0.9032, Validation Accuracy: 0.8701\n","Epoch 8/20, Loss: 0.3025, Training Accuracy: 0.9079, Validation Accuracy: 0.8727\n","Epoch 9/20, Loss: 0.5753, Training Accuracy: 0.9068, Validation Accuracy: 0.7758\n","Epoch 10/20, Loss: 0.5369, Training Accuracy: 0.9159, Validation Accuracy: 0.8182\n","Epoch 11/20, Loss: 0.4524, Training Accuracy: 0.9165, Validation Accuracy: 0.8320\n","Early stopping triggered. Training stopped.\n"]}]},{"cell_type":"markdown","source":["#### Trying with fake news dataset"],"metadata":{"id":"o2hLslwaI7V2"}},{"cell_type":"code","source":["train_csv_url = \"/content/drive/Shareddrives/test/FYP/fake-news/train.csv\"\n","train_data = pd.read_csv(train_csv_url)\n","train_data.head()\n","\n","filtered_train_data = train_data.copy()\n","\n","# Remove missing values in \"text\" column\n","print(f\"missing value count {filtered_train_data['text'].isna().sum()}\")\n","filtered_train_data.dropna(subset=['text'], inplace=True)\n","\n","# Check for empty strings and drop rows with empty \"text\" values\n","filtered_train_data['text'] = filtered_train_data['text'].str.strip() # Strip whitespace from the \"text\" column\n","print(f\"empty string count {filtered_train_data[filtered_train_data['text'] == ''].shape[0]}\")\n","filtered_train_data = filtered_train_data[filtered_train_data['text'] != '']\n","print(f\"filtered_train_data dataset shape {filtered_train_data.shape}\")\n","\n","x_train, x_temp, y_train, y_temp = train_test_split(filtered_train_data['text'], filtered_train_data['label'], test_size=0.2, stratify=filtered_train_data['label'], random_state=2023)\n","x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=2023)\n","\n","print(f\"y_train\\n {y_train.value_counts()}\")\n","print(f\"y_test\\n {y_test.value_counts()}\")\n","print(f\"y_val\\n {y_val.value_counts()}\")\n","\n","# Create DataLoader for training and validation sets\n","batch_size = 128\n","\n","train_dataset = list(zip(x_train, y_train))\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","val_dataset = list(zip(x_val, y_val))\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Initialize the model\n","model = SentimentAnalysisModel(device)\n","\n","# Loss function and optimizer\n","criterion = nn.BCELoss().to(device=device)  # Binary Cross-Entropy Loss for binary classification\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","d_r = 10**-10\n","scheduler = ExponentialLR(optimizer, gamma=(1.0 - d_r))\n","\n","# Define early stopping parameters\n","patience = 5\n","best_validation_accuracy = 0\n","no_improvement_counter = 0\n","\n","# Training loop\n","num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_correct_train = 0\n","    total_samples_train = 0\n","\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","\n","        # Compute training accuracy\n","        predicted_labels_train = (outputs > 0.5).int()\n","        total_correct_train += (predicted_labels_train.view(-1) == labels.to(device=device)).sum().item()\n","\n","        # Accumulate the sum of batch sizes\n","        total_samples_train += len(labels)\n","\n","        loss = criterion(outputs, torch.as_tensor(labels, dtype=torch.float32).unsqueeze(1).to(device=device))\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Compute training accuracy\n","    train_accuracy = total_correct_train / total_samples_train\n","\n","    # Validation\n","    val_loss = 0.0\n","    total_correct_val = 0\n","    total_samples_val = 0\n","    num_iterations = (len(y_val) + batch_size - 1) // batch_size\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for val_inputs, val_labels in val_loader:\n","            val_outputs = model(val_inputs)\n","\n","            # Compute validation accuracy\n","            predicted_labels_val = (val_outputs > 0.5).int()\n","            total_correct_val += (predicted_labels_val.view(-1) == val_labels.to(device=device)).sum().item()\n","\n","            # Accumulate the sum of batch sizes\n","            total_samples_val += len(val_labels)\n","\n","            val_batch_targets = torch.as_tensor(val_labels, dtype=torch.float32).unsqueeze(1).to(device=device)\n","            ce = criterion(val_outputs, val_batch_targets).item()\n","            val_loss += ce\n","\n","        avg_val_loss = val_loss / len(val_loader)\n","        val_accuracy = total_correct_val / total_samples_val\n","\n","        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_val_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","        # Check for early stopping\n","        if val_accuracy > best_validation_accuracy:\n","            best_validation_accuracy = val_accuracy\n","            no_improvement_counter = 0\n","            # Save the trained best model if needed\n","            torch.save(model.state_dict(), '/content/drive/Shareddrives/test/FYP/sentiment/sentiment-analysis-fakenews-model.pth')\n","        else:\n","            no_improvement_counter += 1\n","\n","        # If no improvement for 'patience' consecutive epochs, stop training\n","        if no_improvement_counter >= patience:\n","            print(\"Early stopping triggered. Training stopped.\")\n","            break"],"metadata":{"id":"0dcoMT-kfcSF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702826760529,"user_tz":-330,"elapsed":1887711,"user":{"displayName":"Nimsara Fernando","userId":"05456346427770267598"}},"outputId":"d72a0a62-96e3-430c-999c-22b0d9a3240b"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["missing value count 39\n","empty string count 77\n","filtered_train_data dataset shape (20684, 5)\n","y_train\n"," 0    8309\n","1    8238\n","Name: label, dtype: int64\n","y_test\n"," 0    1039\n","1    1029\n","Name: label, dtype: int64\n","y_val\n"," 0    1039\n","1    1030\n","Name: label, dtype: int64\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Loss: 0.2202, Training Accuracy: 0.8494, Validation Accuracy: 0.9101\n","Epoch 2/20, Loss: 0.1494, Training Accuracy: 0.9506, Validation Accuracy: 0.9386\n","Epoch 3/20, Loss: 0.2098, Training Accuracy: 0.9706, Validation Accuracy: 0.9343\n","Epoch 4/20, Loss: 0.1469, Training Accuracy: 0.9783, Validation Accuracy: 0.9435\n","Epoch 5/20, Loss: 0.1233, Training Accuracy: 0.9854, Validation Accuracy: 0.9546\n","Epoch 6/20, Loss: 0.1233, Training Accuracy: 0.9896, Validation Accuracy: 0.9560\n","Epoch 7/20, Loss: 0.1397, Training Accuracy: 0.9912, Validation Accuracy: 0.9575\n","Epoch 8/20, Loss: 0.5020, Training Accuracy: 0.9934, Validation Accuracy: 0.8951\n","Epoch 9/20, Loss: 0.0958, Training Accuracy: 0.9905, Validation Accuracy: 0.9700\n","Epoch 10/20, Loss: 0.2058, Training Accuracy: 0.9952, Validation Accuracy: 0.9507\n","Epoch 11/20, Loss: 0.1579, Training Accuracy: 0.9924, Validation Accuracy: 0.9468\n","Epoch 12/20, Loss: 0.1223, Training Accuracy: 0.9937, Validation Accuracy: 0.9594\n","Epoch 13/20, Loss: 0.1918, Training Accuracy: 0.9951, Validation Accuracy: 0.9512\n","Epoch 14/20, Loss: 0.1308, Training Accuracy: 0.9941, Validation Accuracy: 0.9613\n","Early stopping triggered. Training stopped.\n"]}]},{"cell_type":"markdown","source":["#### IMDB Dataset"],"metadata":{"id":"5qm5KPBwotn7"}},{"cell_type":"code","source":["file_path = '/content/drive/Shareddrives/test/Sentiment/IMDB Dataset.csv'\n","\n","df = pd.read_csv(file_path, usecols=['sentiment', 'review'])\n","df.dropna(subset=['review'], inplace=True)\n","df.dropna(subset=['sentiment'], inplace=True)\n","\n","df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n","\n","valid_sentiments = [0, 1]\n","df_valid = df[df['sentiment'].isin(valid_sentiments)]\n","df_valid['sentiment'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gY7417uIorv3","executionInfo":{"status":"ok","timestamp":1702834146485,"user_tz":-330,"elapsed":4818,"user":{"displayName":"Nimsara Fernando","userId":"05456346427770267598"}},"outputId":"2466c6f3-f494-4ea8-bade-5136c4a62c28"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    25000\n","0    25000\n","Name: sentiment, dtype: int64"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["x_train, x_temp, y_train, y_temp = train_test_split(df_valid['review'], df_valid['sentiment'], test_size=0.2, stratify=df_valid['sentiment'], random_state=2023)\n","x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=2023)\n","\n","print(f\"y_train\\n {y_train.value_counts()}\")\n","print(f\"y_test\\n {y_test.value_counts()}\")\n","print(f\"y_val\\n {y_val.value_counts()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QRXHBJ5co7uq","executionInfo":{"status":"ok","timestamp":1702834147083,"user_tz":-330,"elapsed":601,"user":{"displayName":"Nimsara Fernando","userId":"05456346427770267598"}},"outputId":"f145e40a-03be-4d64-e532-4d06497ac4a1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["y_train\n"," 1    20000\n","0    20000\n","Name: sentiment, dtype: int64\n","y_test\n"," 1    2500\n","0    2500\n","Name: sentiment, dtype: int64\n","y_val\n"," 1    2500\n","0    2500\n","Name: sentiment, dtype: int64\n"]}]},{"cell_type":"code","source":["# Create DataLoader for training and validation sets\n","batch_size = 128\n","\n","train_dataset = list(zip(x_train, y_train))\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","val_dataset = list(zip(x_val, y_val))\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Initialize the model\n","model = SentimentAnalysisModel(device)\n","\n","# Loss function and optimizer\n","criterion = nn.BCELoss().to(device=device)  # Binary Cross-Entropy Loss for binary classification\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","d_r = 10**-10\n","scheduler = ExponentialLR(optimizer, gamma=(1.0 - d_r))\n","\n","# Define early stopping parameters\n","patience = 5\n","best_validation_accuracy = 0\n","no_improvement_counter = 0\n","\n","# Training loop\n","num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_correct_train = 0\n","    total_samples_train = 0\n","\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","\n","        # Compute training accuracy\n","        predicted_labels_train = (outputs > 0.5).int()\n","        total_correct_train += (predicted_labels_train.view(-1) == labels.to(device=device)).sum().item()\n","\n","        # Accumulate the sum of batch sizes\n","        total_samples_train += len(labels)\n","\n","        loss = criterion(outputs, torch.as_tensor(labels, dtype=torch.float32).unsqueeze(1).to(device=device))\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Compute training accuracy\n","    train_accuracy = total_correct_train / total_samples_train\n","\n","    # Validation\n","    val_loss = 0.0\n","    total_correct_val = 0\n","    total_samples_val = 0\n","    num_iterations = (len(y_val) + batch_size - 1) // batch_size\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for val_inputs, val_labels in val_loader:\n","            val_outputs = model(val_inputs)\n","\n","            # Compute validation accuracy\n","            predicted_labels_val = (val_outputs > 0.5).int()\n","            total_correct_val += (predicted_labels_val.view(-1) == val_labels.to(device=device)).sum().item()\n","\n","            # Accumulate the sum of batch sizes\n","            total_samples_val += len(val_labels)\n","\n","            val_batch_targets = torch.as_tensor(val_labels, dtype=torch.float32).unsqueeze(1).to(device=device)\n","            ce = criterion(val_outputs, val_batch_targets).item()\n","            val_loss += ce\n","\n","        avg_val_loss = val_loss / len(val_loader)\n","        val_accuracy = total_correct_val / total_samples_val\n","\n","        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_val_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","        # Check for early stopping\n","        if val_accuracy > best_validation_accuracy:\n","            best_validation_accuracy = val_accuracy\n","            no_improvement_counter = 0\n","            # Save the trained best model if needed\n","            torch.save(model.state_dict(), '/content/drive/Shareddrives/test/FYP/sentiment/sentiment-analysis-imdb-all-model.pth')\n","        else:\n","            no_improvement_counter += 1\n","\n","        # If no improvement for 'patience' consecutive epochs, stop training\n","        if no_improvement_counter >= patience:\n","            print(\"Early stopping triggered. Training stopped.\")\n","            break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UnoS8WLjqFb-","outputId":"1970e74d-1b25-47b5-9c7d-baca2ae3523b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Loss: 0.3891, Training Accuracy: 0.7524, Validation Accuracy: 0.8302\n","Epoch 2/20, Loss: 0.3451, Training Accuracy: 0.8509, Validation Accuracy: 0.8552\n","Epoch 3/20, Loss: 0.3520, Training Accuracy: 0.8794, Validation Accuracy: 0.8512\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OxrmHU7nq7BN"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["07vPI8sFz6c1"],"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}